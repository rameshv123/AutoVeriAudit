# AutoVeriAudit
Automated Vulnerability Analysis and Security Reporting Framework for Blockchain Smart Contracts
AutoVeriAudit is a research-oriented automated framework designed to enhance smart contract security through structured vulnerability analysis, severity modeling, and automated security reporting. The system integrates verification outputs with intelligent analysis modules to generate explainable risk scores, remediation recommendations, and portfolio-level security insights.
This repository provides a fully reproducible SCI-ready implementation aligned with formal research methodology, enabling large-scale automated auditing workflows.

##  ğŸ¯ Key Features

â€¢	Automated batch analysis of multiple smart contracts

â€¢	Feature-based vulnerability severity scoring

â€¢	Dependency-aware risk propagation

â€¢	Explainable scoring with component breakdown

â€¢	Rule-based remediation recommendation engine

â€¢	Multi-format report generation:


o	PDF

o	HTML

o	JSON (API-ready)

â€¢	Portfolio-level dashboard analytics

â€¢	Benchmarking and runtime evaluation

â€¢	Configurable and reproducible pipeline

ğŸ§© System Architecture Overview
AutoVeriAudit follows a layered architecture:
1.	Input Acquisition Layer
Handles contract ingestion, metadata extraction, and batch scheduling.
2.	Verification Integration Layer
Consumes vulnerability outputs from external verification engines.
3.	Automation and Intelligence Layer
Performs feature extraction, severity modeling, classification, and dependency analysis.
4.	Reporting and Delivery Layer
Generates structured reports, dashboards, and machine-readable outputs.

ğŸ“‚ Repository Structure
AutoVeriAudit/
â”‚

â”œâ”€â”€ core/

â”‚       â”œâ”€â”€ ingestion/        # Contract loading and scheduling

â”‚       â”œâ”€â”€ verification/     # VeriChain interface wrapper

â”‚       â”œâ”€â”€ analysis/         # Feature extraction and aggregation

â”‚       â”œâ”€â”€ scoring/          # Severity modeling and classification

â”‚       â”œâ”€â”€ remediation/      # Knowledge base and fix engine

â”‚       â”œâ”€â”€ reporting/        # Report and dashboard generation

â”‚       â””â”€â”€ benchmarking/     # Runtime evaluation tools
â”‚

â”œâ”€â”€ pipeline/

â”‚       â””â”€â”€ run_pipeline.py   # Main execution workflow
â”‚

â”œâ”€â”€ config/               # Thresholds, weights, schema

â”œâ”€â”€ templates/            # Report templates

â”œâ”€â”€ data/                 # Contracts and outputs

â”œâ”€â”€ utils/                # Logging and helper utilities

â””â”€â”€ tests/                # Smoke test

âš™ï¸ Installation
Create a virtual environment and install dependencies:
python -m venv .venv
Windows:
.venv\Scripts\activate
Linux/Mac:
source .venv/bin/activate
Install requirements:
pip install -r requirements.txt

â–¶ï¸ Running the Framework
Execute the automated pipeline:
python pipeline/run_pipeline.py --contracts_dir data/contracts
The framework will automatically:
â€¢	Load contracts
â€¢	Run vulnerability analysis
â€¢	Compute severity scores
â€¢	Generate remediation recommendations
â€¢	Produce reports and dashboard

ğŸ“Š Output Files
After execution, results will be generated in:
data/outputs/
Folder	Description
reports	Contract-level PDF and HTML reports
dashboards	Portfolio-level analytics dashboard
json	Machine-readable outputs and manifest
benchmarks	Runtime performance metrics

ğŸ§  Verification Engine Integration
The current implementation includes a mock verification adapter for demonstration purposes.
To integrate a real formal verification tool:
Edit:
core/verification/verichain_interface.py
Replace:
run_verification(contract)
with your verification engineâ€™s output logic.
Expected output:
List[Vulnerability]
Each vulnerability should include:
â€¢	type
â€¢	location
â€¢	description
â€¢	execution trace

ğŸ”¬ Reproducibility and Configuration
All analytical parameters are externally configurable:
config/weights.yaml       # Severity scoring weights
config/thresholds.yaml    # Risk classification thresholds
config/pipeline_config.yaml
This ensures experiments remain fully reproducible.

ğŸ“ˆ Benchmarking Support
AutoVeriAudit automatically records execution timing metrics:
data/outputs/benchmarks/timings.json
These metrics can be used in experimental evaluation sections to report:
â€¢	scalability
â€¢	runtime efficiency
â€¢	automation performance

ğŸ§ª Continuous Integration
The repository includes a GitHub Actions workflow that performs:
â€¢	Dependency installation
â€¢	Pipeline smoke testing
â€¢	Output validation

ğŸ›¡ï¸ Research Scope
This implementation focuses on automated vulnerability analysis and reporting.
Formal verification itself is treated as an external module to maintain modular research design.

ğŸ“„ Citation
If you use this framework in academic work, please cite:
AutoVeriAudit: An Automated Vulnerability Analysis and Security Reporting Framework for Blockchain Smart Contracts (full paper URL will be provided after publication)
